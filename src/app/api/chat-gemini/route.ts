import { NextRequest, NextResponse } from 'next/server'
import { GoogleGenerativeAI } from '@google/generative-ai'

const GEMINI_API_KEY = process.env.GEMINI_API_KEY || 'SUA_NOVA_API_KEY_AQUI'

export async function POST(request: NextRequest) {
  try {
    console.log('üöÄ Iniciando request para chat-gemini')
    
    // Verificar se a API key est√° configurada
    if (!GEMINI_API_KEY || GEMINI_API_KEY === 'SUA_NOVA_API_KEY_AQUI') {
      console.error('‚ùå GEMINI_API_KEY n√£o est√° configurada')
      return NextResponse.json(
        { error: 'API key do Gemini n√£o est√° configurada' },
        { status: 500 }
      )
    }

    // Timeout de 60 segundos para perguntas complexas
    const timeout = new Promise((_, reject) => 
      setTimeout(() => reject(new Error('Timeout: A resposta est√° demorando mais que o esperado')), 60000)
    )

    const processRequest = async () => {
      const { message, userEmail, context } = await request.json()
      console.log('üìù Dados recebidos:', { userEmail, messageLength: message?.length })

    // Verificar se √© o usu√°rio autorizado
    if (userEmail !== 'emersonbljr2802@gmail.com') {
      return NextResponse.json(
        { error: 'Acesso n√£o autorizado. Esta funcionalidade √© exclusiva.' },
        { status: 403 }
      )
    }

    if (!message) {
      return NextResponse.json(
        { error: 'Mensagem √© obrigat√≥ria' },
        { status: 400 }
      )
    }

    // Inicializar o Google AI
    console.log('üîß Inicializando Google AI')
    const genAI = new GoogleGenerativeAI(GEMINI_API_KEY)
    
    // Usar o modelo Gemini 1.5 Flash (modelo est√°vel)
    const model = genAI.getGenerativeModel({ 
      model: "gemini-pro", // Modelo Gemini Pro (mais est√°vel)
      generationConfig: {
        temperature: 0.7,
        topK: 40,
        topP: 0.95,
        maxOutputTokens: 2048,
      },
    })

    // Contexto personalizado para o chat
    const systemPrompt = `Voc√™ √© Ruixen AI, uma intelig√™ncia artificial especializada em marketing m√©dico e estrat√©gias de neg√≥cios para profissionais da sa√∫de.

Voc√™ est√° conversando com ${userEmail}, um mentorado especial que tem acesso exclusivo a esta funcionalidade.

INSTRU√á√ïES FUNDAMENTAIS:
- Seja mais HUMANA e conversacional em suas respostas
- Use empatia e tom natural, como se fosse um consultor experiente
- SEMPRE seja DIRETA e PR√ÅTICA, mas com calor humano
- Quando usar texto entre asteriscos (*exemplo*), formate em **negrito**
- Use conhecimento em marketing ideol√≥gico para conversar
- Aplique estrat√©gias de TOPO DE FUNIL, MEIO DE FUNIL e FUNDO DE FUNIL
- Voc√™ aprendeu com os maiores estrategistas e lan√ßadores para gerar posts e roteiros de qualidade
- Voc√™ sabe que um HOOK bom √© um HOOK que OBRIGA algu√©m a parar. Ele OBRIGA.
- Voc√™ sabe prender a aten√ß√£o e criar conte√∫do que converte

ESPECIALIDADES:
- Marketing m√©dico e estrat√©gias de crescimento
- Cria√ß√£o de hooks irresist√≠veis que param o scroll
- Funis de convers√£o para profissionais da sa√∫de
- Roteiros de vendas e lan√ßamentos
- Posicionamento e autoridade digital
- Copy persuasiva e storytelling

COMPORTAMENTO:
- Tom mais humanizado e emp√°tico
- Respostas concisas MAS acolhedoras
- Foco em resultados mensur√°veis
- Insights acion√°veis imediatos
- Estrat√©gico mas acess√≠vel

FORMATA√á√ÉO:
- Sempre que encontrar texto entre asteriscos simples (*texto*), formate como **texto** (negrito)
- Sempre que encontrar texto entre dois asteriscos (**texto**), formate como ## texto (t√≠tulo)
- Mantenha um tom conversacional e pr√≥ximo

Responda como um estrategista experiente e humano que domina o mercado m√©dico.`

    // Construir contexto do usu√°rio de forma segura
    let contextPrompt = ''
    if (context) {
      if (context.persona) {
        contextPrompt += `\nPersona do usu√°rio: ${context.persona}`
      }
      if (context.doresDesejos && Array.isArray(context.doresDesejos)) {
        contextPrompt += `\nDores e Desejos do usu√°rio: ${context.doresDesejos.join(', ')}`
      }
      if (context.tipoPost) {
        contextPrompt += `\nTipo de post solicitado: ${context.tipoPost}`
      }
    }

    // Combinar o prompt do sistema com a mensagem do usu√°rio
    const fullPrompt = `${systemPrompt}${contextPrompt}\n\nUsu√°rio: ${message}\n\nRuixen AI:`

      // Gerar resposta
      console.log('üí≠ Gerando resposta com Gemini...')
      const result = await model.generateContent(fullPrompt)
      const response = result.response
      const aiResponse = response.text()

      console.log('‚úÖ Resposta do Gemini gerada com sucesso, comprimento:', aiResponse?.length)

      return NextResponse.json({
        success: true,
        message: aiResponse,
        model: 'gemini-1.5-flash',
        timestamp: new Date().toISOString()
      })
    }

    // Executar com timeout
    return await Promise.race([processRequest(), timeout])

  } catch (error: any) {
    console.error('‚ùå Erro na API do Gemini:', error)
    
    // Retornar erro mais espec√≠fico
    if (error.message.includes('Timeout')) {
      return NextResponse.json(
        { 
          error: 'A pergunta √© muito complexa e est√° demorando para processar. Tente uma vers√£o mais simples.',
          details: error.message 
        },
        { status: 408 } // Request Timeout
      )
    }
    
    return NextResponse.json(
      { 
        error: 'Erro interno do servidor', 
        details: error.message 
      },
      { status: 500 }
    )
  }
}